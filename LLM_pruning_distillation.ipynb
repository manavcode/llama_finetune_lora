{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c17c89b1-8f75-476f-9081-ee9108e96c9c",
      "metadata": {
        "id": "c17c89b1-8f75-476f-9081-ee9108e96c9c"
      },
      "outputs": [],
      "source": [
        "# !pip install accelerate -U\n",
        "# !pip install install torch torchvision torchaudio --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed1e0561-9fd6-4c8f-8463-923a941344b6",
      "metadata": {
        "id": "ed1e0561-9fd6-4c8f-8463-923a941344b6"
      },
      "outputs": [],
      "source": [
        "# !pip install huggingface_hub\n",
        "!python -c \"from huggingface_hub.hf_api import HfFolder; HfFolder.save_token('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')\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78b412e3-4572-4429-aa50-fe4b9995e748",
      "metadata": {
        "id": "78b412e3-4572-4429-aa50-fe4b9995e748"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "dataset = load_dataset(\"glue\", \"rte\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "265507c3-f484-4e2d-841f-8253b78956f5",
      "metadata": {
        "id": "265507c3-f484-4e2d-841f-8253b78956f5",
        "outputId": "72a16b18-a353-407e-de6b-745754e1eac1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'sentence1': 'No Weapons of Mass Destruction Found in Iraq Yet.', 'sentence2': 'Weapons of Mass Destruction Found in Iraq.', 'label': 1, 'idx': 0}\n"
          ]
        }
      ],
      "source": [
        "print(dataset['train'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61d7ede7-68c8-48ec-baf2-f33a0aaddae5",
      "metadata": {
        "id": "61d7ede7-68c8-48ec-baf2-f33a0aaddae5"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['sentence1'], examples['sentence2'], padding=\"max_length\", truncation=True, max_length=512)\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85a0a8ac-2049-4c02-a09d-284e1890ceec",
      "metadata": {
        "id": "85a0a8ac-2049-4c02-a09d-284e1890ceec"
      },
      "outputs": [],
      "source": [
        "from datasets import load_metric\n",
        "import numpy as np\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    metric = load_metric(\"accuracy\")\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "211a1286-8dd6-44d7-99a9-ff7da63f21c2",
      "metadata": {
        "id": "211a1286-8dd6-44d7-99a9-ff7da63f21c2",
        "outputId": "f939eb45-d331-474d-e5fe-9dcd7e78820c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertForSequenceClassification, TrainingArguments, Trainer\n",
        "import torch\n",
        "\n",
        "# Check CUDA availability\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# device=\"cuda:0\"\n",
        "\n",
        "# Load the model\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "feb55bf0-2c85-4eef-abab-f0f532eff360",
      "metadata": {
        "scrolled": true,
        "id": "feb55bf0-2c85-4eef-abab-f0f532eff360",
        "outputId": "62b092f7-f40b-4ca0-db0d-06fc786dde19"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
            "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running training *****\n",
            "  Num examples = 2,490\n",
            "  Num Epochs = 30\n",
            "  Instantaneous batch size per device = 64\n",
            "  Training with DataParallel so batch size has been adjusted to: 512\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 512\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 150\n",
            "  Number of trainable parameters = 109,483,778\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [150/150 05:45, Epoch 30/30]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.749554</td>\n",
              "      <td>0.527076</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.777700</td>\n",
              "      <td>0.745954</td>\n",
              "      <td>0.527076</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.777700</td>\n",
              "      <td>0.739669</td>\n",
              "      <td>0.527076</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.761300</td>\n",
              "      <td>0.730535</td>\n",
              "      <td>0.530686</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.761300</td>\n",
              "      <td>0.718336</td>\n",
              "      <td>0.534296</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.736800</td>\n",
              "      <td>0.703721</td>\n",
              "      <td>0.530686</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.736800</td>\n",
              "      <td>0.689945</td>\n",
              "      <td>0.541516</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.702500</td>\n",
              "      <td>0.678946</td>\n",
              "      <td>0.577617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.702500</td>\n",
              "      <td>0.678548</td>\n",
              "      <td>0.570397</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.673100</td>\n",
              "      <td>0.676953</td>\n",
              "      <td>0.577617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.673100</td>\n",
              "      <td>0.674424</td>\n",
              "      <td>0.599278</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.653300</td>\n",
              "      <td>0.670230</td>\n",
              "      <td>0.588448</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.653300</td>\n",
              "      <td>0.670113</td>\n",
              "      <td>0.588448</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.612600</td>\n",
              "      <td>0.674155</td>\n",
              "      <td>0.613718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.612600</td>\n",
              "      <td>0.670533</td>\n",
              "      <td>0.592058</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.559200</td>\n",
              "      <td>0.676065</td>\n",
              "      <td>0.613718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.559200</td>\n",
              "      <td>0.673101</td>\n",
              "      <td>0.592058</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.501400</td>\n",
              "      <td>0.680938</td>\n",
              "      <td>0.613718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.501400</td>\n",
              "      <td>0.684043</td>\n",
              "      <td>0.617329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.434300</td>\n",
              "      <td>0.691564</td>\n",
              "      <td>0.620939</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.434300</td>\n",
              "      <td>0.704644</td>\n",
              "      <td>0.635379</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.354300</td>\n",
              "      <td>0.728505</td>\n",
              "      <td>0.631769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.354300</td>\n",
              "      <td>0.741158</td>\n",
              "      <td>0.635379</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.280100</td>\n",
              "      <td>0.772334</td>\n",
              "      <td>0.624549</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.280100</td>\n",
              "      <td>0.795891</td>\n",
              "      <td>0.631769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.209200</td>\n",
              "      <td>0.834211</td>\n",
              "      <td>0.649819</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.209200</td>\n",
              "      <td>0.857213</td>\n",
              "      <td>0.657040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.152600</td>\n",
              "      <td>0.889046</td>\n",
              "      <td>0.646209</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.152600</td>\n",
              "      <td>0.914677</td>\n",
              "      <td>0.642599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.105200</td>\n",
              "      <td>0.966403</td>\n",
              "      <td>0.646209</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 512\n",
            "/tmp/ipykernel_901335/915183798.py:5: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(\"accuracy\")\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results/checkpoint-5\n",
            "Configuration saved in ./results/checkpoint-5/config.json\n",
            "Model weights saved in ./results/checkpoint-5/model.safetensors\n",
            "tokenizer config file saved in ./results/checkpoint-5/tokenizer_config.json\n",
            "Special tokens file saved in ./results/checkpoint-5/special_tokens_map.json\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 512\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results/checkpoint-10\n",
            "Configuration saved in ./results/checkpoint-10/config.json\n",
            "Model weights saved in ./results/checkpoint-10/model.safetensors\n",
            "tokenizer config file saved in ./results/checkpoint-10/tokenizer_config.json\n",
            "Special tokens file saved in ./results/checkpoint-10/special_tokens_map.json\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 512\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results/checkpoint-15\n",
            "Configuration saved in ./results/checkpoint-15/config.json\n",
            "Model weights saved in ./results/checkpoint-15/model.safetensors\n",
            "tokenizer config file saved in ./results/checkpoint-15/tokenizer_config.json\n",
            "Special tokens file saved in ./results/checkpoint-15/special_tokens_map.json\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 512\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results/checkpoint-20\n",
            "Configuration saved in ./results/checkpoint-20/config.json\n",
            "Model weights saved in ./results/checkpoint-20/model.safetensors\n",
            "tokenizer config file saved in ./results/checkpoint-20/tokenizer_config.json\n",
            "Special tokens file saved in ./results/checkpoint-20/special_tokens_map.json\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 512\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results/checkpoint-25\n",
            "Configuration saved in ./results/checkpoint-25/config.json\n",
            "Model weights saved in ./results/checkpoint-25/model.safetensors\n",
            "tokenizer config file saved in ./results/checkpoint-25/tokenizer_config.json\n",
            "Special tokens file saved in ./results/checkpoint-25/special_tokens_map.json\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 512\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results/checkpoint-30\n",
            "Configuration saved in ./results/checkpoint-30/config.json\n",
            "Model weights saved in ./results/checkpoint-30/model.safetensors\n",
            "tokenizer config file saved in ./results/checkpoint-30/tokenizer_config.json\n",
            "Special tokens file saved in ./results/checkpoint-30/special_tokens_map.json\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 512\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results/checkpoint-35\n",
            "Configuration saved in ./results/checkpoint-35/config.json\n",
            "Model weights saved in ./results/checkpoint-35/model.safetensors\n",
            "tokenizer config file saved in ./results/checkpoint-35/tokenizer_config.json\n",
            "Special tokens file saved in ./results/checkpoint-35/special_tokens_map.json\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 512\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results/checkpoint-40\n",
            "Configuration saved in ./results/checkpoint-40/config.json\n",
            "Model weights saved in ./results/checkpoint-40/model.safetensors\n",
            "tokenizer config file saved in ./results/checkpoint-40/tokenizer_config.json\n",
            "Special tokens file saved in ./results/checkpoint-40/special_tokens_map.json\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 512\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results/checkpoint-45\n",
            "Configuration saved in ./results/checkpoint-45/config.json\n",
            "Model weights saved in ./results/checkpoint-45/model.safetensors\n",
            "tokenizer config file saved in ./results/checkpoint-45/tokenizer_config.json\n",
            "Special tokens file saved in ./results/checkpoint-45/special_tokens_map.json\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 512\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results/checkpoint-50\n",
            "Configuration saved in ./results/checkpoint-50/config.json\n",
            "Model weights saved in ./results/checkpoint-50/model.safetensors\n",
            "tokenizer config file saved in ./results/checkpoint-50/tokenizer_config.json\n",
            "Special tokens file saved in ./results/checkpoint-50/special_tokens_map.json\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 512\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results/checkpoint-55\n",
            "Configuration saved in ./results/checkpoint-55/config.json\n",
            "Model weights saved in ./results/checkpoint-55/model.safetensors\n",
            "tokenizer config file saved in ./results/checkpoint-55/tokenizer_config.json\n",
            "Special tokens file saved in ./results/checkpoint-55/special_tokens_map.json\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 512\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results/checkpoint-60\n",
            "Configuration saved in ./results/checkpoint-60/config.json\n",
            "Model weights saved in ./results/checkpoint-60/model.safetensors\n",
            "tokenizer config file saved in ./results/checkpoint-60/tokenizer_config.json\n",
            "Special tokens file saved in ./results/checkpoint-60/special_tokens_map.json\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 512\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results/checkpoint-65\n",
            "Configuration saved in ./results/checkpoint-65/config.json\n",
            "Model weights saved in ./results/checkpoint-65/model.safetensors\n",
            "tokenizer config file saved in ./results/checkpoint-65/tokenizer_config.json\n",
            "Special tokens file saved in ./results/checkpoint-65/special_tokens_map.json\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 512\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results/checkpoint-70\n",
            "Configuration saved in ./results/checkpoint-70/config.json\n",
            "Model weights saved in ./results/checkpoint-70/model.safetensors\n",
            "tokenizer config file saved in ./results/checkpoint-70/tokenizer_config.json\n",
            "Special tokens file saved in ./results/checkpoint-70/special_tokens_map.json\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 512\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results/checkpoint-75\n",
            "Configuration saved in ./results/checkpoint-75/config.json\n",
            "Model weights saved in ./results/checkpoint-75/model.safetensors\n",
            "tokenizer config file saved in ./results/checkpoint-75/tokenizer_config.json\n",
            "Special tokens file saved in ./results/checkpoint-75/special_tokens_map.json\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 512\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results/checkpoint-80\n",
            "Configuration saved in ./results/checkpoint-80/config.json\n",
            "Model weights saved in ./results/checkpoint-80/model.safetensors\n",
            "tokenizer config file saved in ./results/checkpoint-80/tokenizer_config.json\n",
            "Special tokens file saved in ./results/checkpoint-80/special_tokens_map.json\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 512\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results/checkpoint-85\n",
            "Configuration saved in ./results/checkpoint-85/config.json\n",
            "Model weights saved in ./results/checkpoint-85/model.safetensors\n",
            "tokenizer config file saved in ./results/checkpoint-85/tokenizer_config.json\n",
            "Special tokens file saved in ./results/checkpoint-85/special_tokens_map.json\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 512\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results/checkpoint-90\n",
            "Configuration saved in ./results/checkpoint-90/config.json\n",
            "Model weights saved in ./results/checkpoint-90/model.safetensors\n",
            "tokenizer config file saved in ./results/checkpoint-90/tokenizer_config.json\n",
            "Special tokens file saved in ./results/checkpoint-90/special_tokens_map.json\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 512\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results/checkpoint-95\n",
            "Configuration saved in ./results/checkpoint-95/config.json\n",
            "Model weights saved in ./results/checkpoint-95/model.safetensors\n",
            "tokenizer config file saved in ./results/checkpoint-95/tokenizer_config.json\n",
            "Special tokens file saved in ./results/checkpoint-95/special_tokens_map.json\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 512\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results/checkpoint-100\n",
            "Configuration saved in ./results/checkpoint-100/config.json\n",
            "Model weights saved in ./results/checkpoint-100/model.safetensors\n",
            "tokenizer config file saved in ./results/checkpoint-100/tokenizer_config.json\n",
            "Special tokens file saved in ./results/checkpoint-100/special_tokens_map.json\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 512\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results/checkpoint-105\n",
            "Configuration saved in ./results/checkpoint-105/config.json\n",
            "Model weights saved in ./results/checkpoint-105/model.safetensors\n",
            "tokenizer config file saved in ./results/checkpoint-105/tokenizer_config.json\n",
            "Special tokens file saved in ./results/checkpoint-105/special_tokens_map.json\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 512\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results/checkpoint-110\n",
            "Configuration saved in ./results/checkpoint-110/config.json\n",
            "Model weights saved in ./results/checkpoint-110/model.safetensors\n",
            "tokenizer config file saved in ./results/checkpoint-110/tokenizer_config.json\n",
            "Special tokens file saved in ./results/checkpoint-110/special_tokens_map.json\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 512\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results/checkpoint-115\n",
            "Configuration saved in ./results/checkpoint-115/config.json\n",
            "Model weights saved in ./results/checkpoint-115/model.safetensors\n",
            "tokenizer config file saved in ./results/checkpoint-115/tokenizer_config.json\n",
            "Special tokens file saved in ./results/checkpoint-115/special_tokens_map.json\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 512\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results/checkpoint-120\n",
            "Configuration saved in ./results/checkpoint-120/config.json\n",
            "Model weights saved in ./results/checkpoint-120/model.safetensors\n",
            "tokenizer config file saved in ./results/checkpoint-120/tokenizer_config.json\n",
            "Special tokens file saved in ./results/checkpoint-120/special_tokens_map.json\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 512\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results/checkpoint-125\n",
            "Configuration saved in ./results/checkpoint-125/config.json\n",
            "Model weights saved in ./results/checkpoint-125/model.safetensors\n",
            "tokenizer config file saved in ./results/checkpoint-125/tokenizer_config.json\n",
            "Special tokens file saved in ./results/checkpoint-125/special_tokens_map.json\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 512\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results/checkpoint-130\n",
            "Configuration saved in ./results/checkpoint-130/config.json\n",
            "Model weights saved in ./results/checkpoint-130/model.safetensors\n",
            "tokenizer config file saved in ./results/checkpoint-130/tokenizer_config.json\n",
            "Special tokens file saved in ./results/checkpoint-130/special_tokens_map.json\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 512\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results/checkpoint-135\n",
            "Configuration saved in ./results/checkpoint-135/config.json\n",
            "Model weights saved in ./results/checkpoint-135/model.safetensors\n",
            "tokenizer config file saved in ./results/checkpoint-135/tokenizer_config.json\n",
            "Special tokens file saved in ./results/checkpoint-135/special_tokens_map.json\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 512\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results/checkpoint-140\n",
            "Configuration saved in ./results/checkpoint-140/config.json\n",
            "Model weights saved in ./results/checkpoint-140/model.safetensors\n",
            "tokenizer config file saved in ./results/checkpoint-140/tokenizer_config.json\n",
            "Special tokens file saved in ./results/checkpoint-140/special_tokens_map.json\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 512\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results/checkpoint-145\n",
            "Configuration saved in ./results/checkpoint-145/config.json\n",
            "Model weights saved in ./results/checkpoint-145/model.safetensors\n",
            "tokenizer config file saved in ./results/checkpoint-145/tokenizer_config.json\n",
            "Special tokens file saved in ./results/checkpoint-145/special_tokens_map.json\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 512\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results/checkpoint-150\n",
            "Configuration saved in ./results/checkpoint-150/config.json\n",
            "Model weights saved in ./results/checkpoint-150/model.safetensors\n",
            "tokenizer config file saved in ./results/checkpoint-150/tokenizer_config.json\n",
            "Special tokens file saved in ./results/checkpoint-150/special_tokens_map.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from ./results/checkpoint-65 (score: 0.6701134443283081).\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=150, training_loss=0.5008951179186503, metrics={'train_runtime': 349.2537, 'train_samples_per_second': 213.885, 'train_steps_per_second': 0.429, 'total_flos': 1.9654395835392e+16, 'train_loss': 0.5008951179186503, 'epoch': 30.0})"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=30,\n",
        "    per_device_train_batch_size=64,\n",
        "    per_device_eval_batch_size=64,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    log_level='info',\n",
        "    load_best_model_at_end=True\n",
        ")\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets['train'],\n",
        "    eval_dataset=tokenized_datasets['validation'],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics  # Add this line to evaluate metrics\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b78bf69-11cd-440a-8941-10ee55577074",
      "metadata": {
        "id": "5b78bf69-11cd-440a-8941-10ee55577074"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.utils.prune as prune\n",
        "import copy\n",
        "\n",
        "def apply_pruning_to_distilbert(model):\n",
        "    for module in model.modules():\n",
        "        if isinstance(module, torch.nn.Linear):\n",
        "            prune.l1_unstructured(module, name='weight', amount=0.5)\n",
        "\n",
        "    return model\n",
        "\n",
        "copied_model = copy.deepcopy(model)\n",
        "pruned_model = apply_pruning_to_distilbert(copied_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d5371a9-20af-426e-88c2-3551ef5b6ee7",
      "metadata": {
        "id": "0d5371a9-20af-426e-88c2-3551ef5b6ee7",
        "outputId": "b9df9f97-5a7a-4f1a-8f55-ebd954b4649e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bert.encoder.layer.0.attention.self.query - Pruned: 294912 of 589824 weights (50.00%)\n",
            "bert.encoder.layer.0.attention.self.key - Pruned: 294912 of 589824 weights (50.00%)\n",
            "bert.encoder.layer.0.attention.self.value - Pruned: 294912 of 589824 weights (50.00%)\n",
            "bert.encoder.layer.0.attention.output.dense - Pruned: 294912 of 589824 weights (50.00%)\n",
            "bert.encoder.layer.0.intermediate.dense - Pruned: 1179648 of 2359296 weights (50.00%)\n",
            "bert.encoder.layer.0.output.dense - Pruned: 1179648 of 2359296 weights (50.00%)\n",
            "bert.encoder.layer.1.attention.self.query - Pruned: 294912 of 589824 weights (50.00%)\n",
            "bert.encoder.layer.1.attention.self.key - Pruned: 294912 of 589824 weights (50.00%)\n",
            "bert.encoder.layer.1.attention.self.value - Pruned: 294912 of 589824 weights (50.00%)\n",
            "bert.encoder.layer.1.attention.output.dense - Pruned: 294912 of 589824 weights (50.00%)\n",
            "bert.encoder.layer.1.intermediate.dense - Pruned: 1179648 of 2359296 weights (50.00%)\n",
            "bert.encoder.layer.1.output.dense - Pruned: 1179648 of 2359296 weights (50.00%)\n",
            "bert.encoder.layer.2.attention.self.query - Pruned: 294912 of 589824 weights (50.00%)\n",
            "bert.encoder.layer.2.attention.self.key - Pruned: 294912 of 589824 weights (50.00%)\n",
            "bert.encoder.layer.2.attention.self.value - Pruned: 294912 of 589824 weights (50.00%)\n",
            "bert.encoder.layer.2.attention.output.dense - Pruned: 294912 of 589824 weights (50.00%)\n",
            "bert.encoder.layer.2.intermediate.dense - Pruned: 1179648 of 2359296 weights (50.00%)\n",
            "bert.encoder.layer.2.output.dense - Pruned: 1179648 of 2359296 weights (50.00%)\n",
            "bert.encoder.layer.3.attention.self.query - Pruned: 294912 of 589824 weights (50.00%)\n",
            "bert.encoder.layer.3.attention.self.key - Pruned: 294912 of 589824 weights (50.00%)\n",
            "bert.encoder.layer.3.attention.self.value - Pruned: 294912 of 589824 weights (50.00%)\n",
            "bert.encoder.layer.3.attention.output.dense - Pruned: 294912 of 589824 weights (50.00%)\n",
            "bert.encoder.layer.3.intermediate.dense - Pruned: 1179648 of 2359296 weights (50.00%)\n",
            "bert.encoder.layer.3.output.dense - Pruned: 1179648 of 2359296 weights (50.00%)\n",
            "bert.encoder.layer.4.attention.self.query - Pruned: 294912 of 589824 weights (50.00%)\n",
            "bert.encoder.layer.4.attention.self.key - Pruned: 294912 of 589824 weights (50.00%)\n",
            "bert.encoder.layer.4.attention.self.value - Pruned: 294912 of 589824 weights (50.00%)\n",
            "bert.encoder.layer.4.attention.output.dense - Pruned: 294912 of 589824 weights (50.00%)\n",
            "bert.encoder.layer.4.intermediate.dense - Pruned: 1179648 of 2359296 weights (50.00%)\n",
            "bert.encoder.layer.4.output.dense - Pruned: 1179648 of 2359296 weights (50.00%)\n",
            "bert.encoder.layer.5.attention.self.query - Pruned: 294912 of 589824 weights (50.00%)\n",
            "bert.encoder.layer.5.attention.self.key - Pruned: 294912 of 589824 weights (50.00%)\n",
            "bert.encoder.layer.5.attention.self.value - Pruned: 294912 of 589824 weights (50.00%)\n",
            "bert.encoder.layer.5.attention.output.dense - Pruned: 294912 of 589824 weights (50.00%)\n",
            "bert.encoder.layer.5.intermediate.dense - Pruned: 1179648 of 2359296 weights (50.00%)\n",
            "bert.encoder.layer.5.output.dense - Pruned: 1179648 of 2359296 weights (50.00%)\n",
            "bert.encoder.layer.6.attention.self.query - Pruned: 294912 of 589824 weights (50.00%)\n",
            "bert.encoder.layer.6.attention.self.key - Pruned: 294912 of 589824 weights (50.00%)\n",
            "bert.encoder.layer.6.attention.self.value - Pruned: 294912 of 589824 weights (50.00%)\n",
            "bert.encoder.layer.6.attention.output.dense - Pruned: 294912 of 589824 weights (50.00%)\n",
            "bert.encoder.layer.6.intermediate.dense - Pruned: 1179648 of 2359296 weights (50.00%)\n",
            "bert.encoder.layer.6.output.dense - Pruned: 1179648 of 2359296 weights (50.00%)\n",
            "bert.encoder.layer.7.attention.self.query - Pruned: 294912 of 589824 weights (50.00%)\n",
            "bert.encoder.layer.7.attention.self.key - Pruned: 294912 of 589824 weights (50.00%)\n",
            "bert.encoder.layer.7.attention.self.value - Pruned: 294912 of 589824 weights (50.00%)\n",
            "bert.encoder.layer.7.attention.output.dense - Pruned: 294912 of 589824 weights (50.00%)\n",
            "bert.encoder.layer.7.intermediate.dense - Pruned: 1179648 of 2359296 weights (50.00%)\n",
            "bert.encoder.layer.7.output.dense - Pruned: 1179648 of 2359296 weights (50.00%)\n",
            "bert.encoder.layer.8.attention.self.query - Pruned: 294912 of 589824 weights (50.00%)\n",
            "bert.encoder.layer.8.attention.self.key - Pruned: 294912 of 589824 weights (50.00%)\n",
            "bert.encoder.layer.8.attention.self.value - Pruned: 294912 of 589824 weights (50.00%)\n",
            "bert.encoder.layer.8.attention.output.dense - Pruned: 294912 of 589824 weights (50.00%)\n",
            "bert.encoder.layer.8.intermediate.dense - Pruned: 1179648 of 2359296 weights (50.00%)\n",
            "bert.encoder.layer.8.output.dense - Pruned: 1179648 of 2359296 weights (50.00%)\n",
            "bert.encoder.layer.9.attention.self.query - Pruned: 294912 of 589824 weights (50.00%)\n",
            "bert.encoder.layer.9.attention.self.key - Pruned: 294912 of 589824 weights (50.00%)\n",
            "bert.encoder.layer.9.attention.self.value - Pruned: 294912 of 589824 weights (50.00%)\n",
            "bert.encoder.layer.9.attention.output.dense - Pruned: 294912 of 589824 weights (50.00%)\n",
            "bert.encoder.layer.9.intermediate.dense - Pruned: 1179648 of 2359296 weights (50.00%)\n",
            "bert.encoder.layer.9.output.dense - Pruned: 1179648 of 2359296 weights (50.00%)\n",
            "bert.encoder.layer.10.attention.self.query - Pruned: 294912 of 589824 weights (50.00%)\n",
            "bert.encoder.layer.10.attention.self.key - Pruned: 294912 of 589824 weights (50.00%)\n",
            "bert.encoder.layer.10.attention.self.value - Pruned: 294912 of 589824 weights (50.00%)\n",
            "bert.encoder.layer.10.attention.output.dense - Pruned: 294912 of 589824 weights (50.00%)\n",
            "bert.encoder.layer.10.intermediate.dense - Pruned: 1179648 of 2359296 weights (50.00%)\n",
            "bert.encoder.layer.10.output.dense - Pruned: 1179648 of 2359296 weights (50.00%)\n",
            "bert.encoder.layer.11.attention.self.query - Pruned: 294912 of 589824 weights (50.00%)\n",
            "bert.encoder.layer.11.attention.self.key - Pruned: 294912 of 589824 weights (50.00%)\n",
            "bert.encoder.layer.11.attention.self.value - Pruned: 294912 of 589824 weights (50.00%)\n",
            "bert.encoder.layer.11.attention.output.dense - Pruned: 294912 of 589824 weights (50.00%)\n",
            "bert.encoder.layer.11.intermediate.dense - Pruned: 1179648 of 2359296 weights (50.00%)\n",
            "bert.encoder.layer.11.output.dense - Pruned: 1179648 of 2359296 weights (50.00%)\n",
            "bert.pooler.dense - Pruned: 294912 of 589824 weights (50.00%)\n",
            "classifier - Pruned: 768 of 1536 weights (50.00%)\n",
            "Overall pruning across linear layers: 50.00% of weights are zero.\n"
          ]
        }
      ],
      "source": [
        "def check_pruning_effectiveness(model):\n",
        "    total_pruned = 0\n",
        "    total_params = 0\n",
        "\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, torch.nn.Linear):\n",
        "            # Get the weight tensor\n",
        "            tensor = module.weight.data\n",
        "            # Count the zeros in the tensor\n",
        "            pruned = torch.sum(tensor == 0)\n",
        "            total_pruned += pruned.item()\n",
        "            total_params += tensor.numel()\n",
        "\n",
        "            # Optional: print details for each layer\n",
        "            print(f\"{name} - Pruned: {pruned.item()} of {tensor.numel()} weights ({100.0 * pruned.item() / tensor.numel():.2f}%)\")\n",
        "\n",
        "    total_pruning_percentage = 100.0 * total_pruned / total_params\n",
        "    print(f\"Overall pruning across linear layers: {total_pruning_percentage:.2f}% of weights are zero.\")\n",
        "\n",
        "check_pruning_effectiveness(pruned_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "252259ab-400d-4e45-ab9a-2ac82d3f931d",
      "metadata": {
        "id": "252259ab-400d-4e45-ab9a-2ac82d3f931d",
        "outputId": "1fe3b957-74ab-4c0c-ec2b-6a92f5f6e2d9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
            "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running training *****\n",
            "  Num examples = 2,490\n",
            "  Num Epochs = 30\n",
            "  Instantaneous batch size per device = 64\n",
            "  Training with DataParallel so batch size has been adjusted to: 512\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 512\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 150\n",
            "  Number of trainable parameters = 109,483,778\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [150/150 06:18, Epoch 30/30]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.679132</td>\n",
              "      <td>0.552347</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.686700</td>\n",
              "      <td>0.678021</td>\n",
              "      <td>0.559567</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.686700</td>\n",
              "      <td>0.676253</td>\n",
              "      <td>0.563177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.680800</td>\n",
              "      <td>0.674429</td>\n",
              "      <td>0.570397</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.680800</td>\n",
              "      <td>0.672812</td>\n",
              "      <td>0.548736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.673700</td>\n",
              "      <td>0.671172</td>\n",
              "      <td>0.552347</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.673700</td>\n",
              "      <td>0.669260</td>\n",
              "      <td>0.563177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.664100</td>\n",
              "      <td>0.667064</td>\n",
              "      <td>0.566787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.664100</td>\n",
              "      <td>0.664254</td>\n",
              "      <td>0.566787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.647100</td>\n",
              "      <td>0.661667</td>\n",
              "      <td>0.563177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.647100</td>\n",
              "      <td>0.660692</td>\n",
              "      <td>0.570397</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.631500</td>\n",
              "      <td>0.661330</td>\n",
              "      <td>0.563177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.631500</td>\n",
              "      <td>0.659419</td>\n",
              "      <td>0.559567</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.606400</td>\n",
              "      <td>0.659727</td>\n",
              "      <td>0.566787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.606400</td>\n",
              "      <td>0.660884</td>\n",
              "      <td>0.577617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.577900</td>\n",
              "      <td>0.664611</td>\n",
              "      <td>0.584838</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.577900</td>\n",
              "      <td>0.661486</td>\n",
              "      <td>0.595668</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.545800</td>\n",
              "      <td>0.664494</td>\n",
              "      <td>0.595668</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.545800</td>\n",
              "      <td>0.665208</td>\n",
              "      <td>0.602888</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.511000</td>\n",
              "      <td>0.668963</td>\n",
              "      <td>0.610108</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.511000</td>\n",
              "      <td>0.674154</td>\n",
              "      <td>0.602888</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.464800</td>\n",
              "      <td>0.684483</td>\n",
              "      <td>0.602888</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.464800</td>\n",
              "      <td>0.695159</td>\n",
              "      <td>0.599278</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.415900</td>\n",
              "      <td>0.707101</td>\n",
              "      <td>0.595668</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.415900</td>\n",
              "      <td>0.719402</td>\n",
              "      <td>0.588448</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.373200</td>\n",
              "      <td>0.730102</td>\n",
              "      <td>0.599278</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.373200</td>\n",
              "      <td>0.746359</td>\n",
              "      <td>0.592058</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.316400</td>\n",
              "      <td>0.764842</td>\n",
              "      <td>0.599278</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.316400</td>\n",
              "      <td>0.779052</td>\n",
              "      <td>0.610108</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.268800</td>\n",
              "      <td>0.800817</td>\n",
              "      <td>0.595668</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 512\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results_pruned/checkpoint-5\n",
            "Configuration saved in ./results_pruned/checkpoint-5/config.json\n",
            "Model weights saved in ./results_pruned/checkpoint-5/model.safetensors\n",
            "tokenizer config file saved in ./results_pruned/checkpoint-5/tokenizer_config.json\n",
            "Special tokens file saved in ./results_pruned/checkpoint-5/special_tokens_map.json\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 512\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results_pruned/checkpoint-10\n",
            "Configuration saved in ./results_pruned/checkpoint-10/config.json\n",
            "Model weights saved in ./results_pruned/checkpoint-10/model.safetensors\n",
            "tokenizer config file saved in ./results_pruned/checkpoint-10/tokenizer_config.json\n",
            "Special tokens file saved in ./results_pruned/checkpoint-10/special_tokens_map.json\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 512\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results_pruned/checkpoint-15\n",
            "Configuration saved in ./results_pruned/checkpoint-15/config.json\n",
            "Model weights saved in ./results_pruned/checkpoint-15/model.safetensors\n",
            "tokenizer config file saved in ./results_pruned/checkpoint-15/tokenizer_config.json\n",
            "Special tokens file saved in ./results_pruned/checkpoint-15/special_tokens_map.json\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 512\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results_pruned/checkpoint-20\n",
            "Configuration saved in ./results_pruned/checkpoint-20/config.json\n",
            "Model weights saved in ./results_pruned/checkpoint-20/model.safetensors\n",
            "tokenizer config file saved in ./results_pruned/checkpoint-20/tokenizer_config.json\n",
            "Special tokens file saved in ./results_pruned/checkpoint-20/special_tokens_map.json\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 512\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results_pruned/checkpoint-25\n",
            "Configuration saved in ./results_pruned/checkpoint-25/config.json\n",
            "Model weights saved in ./results_pruned/checkpoint-25/model.safetensors\n",
            "tokenizer config file saved in ./results_pruned/checkpoint-25/tokenizer_config.json\n",
            "Special tokens file saved in ./results_pruned/checkpoint-25/special_tokens_map.json\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 512\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results_pruned/checkpoint-30\n",
            "Configuration saved in ./results_pruned/checkpoint-30/config.json\n",
            "Model weights saved in ./results_pruned/checkpoint-30/model.safetensors\n",
            "tokenizer config file saved in ./results_pruned/checkpoint-30/tokenizer_config.json\n",
            "Special tokens file saved in ./results_pruned/checkpoint-30/special_tokens_map.json\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 512\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results_pruned/checkpoint-35\n",
            "Configuration saved in ./results_pruned/checkpoint-35/config.json\n",
            "Model weights saved in ./results_pruned/checkpoint-35/model.safetensors\n",
            "tokenizer config file saved in ./results_pruned/checkpoint-35/tokenizer_config.json\n",
            "Special tokens file saved in ./results_pruned/checkpoint-35/special_tokens_map.json\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 512\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results_pruned/checkpoint-40\n",
            "Configuration saved in ./results_pruned/checkpoint-40/config.json\n",
            "Model weights saved in ./results_pruned/checkpoint-40/model.safetensors\n",
            "tokenizer config file saved in ./results_pruned/checkpoint-40/tokenizer_config.json\n",
            "Special tokens file saved in ./results_pruned/checkpoint-40/special_tokens_map.json\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 512\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results_pruned/checkpoint-45\n",
            "Configuration saved in ./results_pruned/checkpoint-45/config.json\n",
            "Model weights saved in ./results_pruned/checkpoint-45/model.safetensors\n",
            "tokenizer config file saved in ./results_pruned/checkpoint-45/tokenizer_config.json\n",
            "Special tokens file saved in ./results_pruned/checkpoint-45/special_tokens_map.json\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 512\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results_pruned/checkpoint-50\n",
            "Configuration saved in ./results_pruned/checkpoint-50/config.json\n",
            "Model weights saved in ./results_pruned/checkpoint-50/model.safetensors\n",
            "tokenizer config file saved in ./results_pruned/checkpoint-50/tokenizer_config.json\n",
            "Special tokens file saved in ./results_pruned/checkpoint-50/special_tokens_map.json\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 512\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results_pruned/checkpoint-55\n",
            "Configuration saved in ./results_pruned/checkpoint-55/config.json\n",
            "Model weights saved in ./results_pruned/checkpoint-55/model.safetensors\n",
            "tokenizer config file saved in ./results_pruned/checkpoint-55/tokenizer_config.json\n",
            "Special tokens file saved in ./results_pruned/checkpoint-55/special_tokens_map.json\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 512\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results_pruned/checkpoint-60\n",
            "Configuration saved in ./results_pruned/checkpoint-60/config.json\n",
            "Model weights saved in ./results_pruned/checkpoint-60/model.safetensors\n",
            "tokenizer config file saved in ./results_pruned/checkpoint-60/tokenizer_config.json\n",
            "Special tokens file saved in ./results_pruned/checkpoint-60/special_tokens_map.json\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 512\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results_pruned/checkpoint-65\n",
            "Configuration saved in ./results_pruned/checkpoint-65/config.json\n",
            "Model weights saved in ./results_pruned/checkpoint-65/model.safetensors\n",
            "tokenizer config file saved in ./results_pruned/checkpoint-65/tokenizer_config.json\n",
            "Special tokens file saved in ./results_pruned/checkpoint-65/special_tokens_map.json\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 512\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results_pruned/checkpoint-70\n",
            "Configuration saved in ./results_pruned/checkpoint-70/config.json\n",
            "Model weights saved in ./results_pruned/checkpoint-70/model.safetensors\n",
            "tokenizer config file saved in ./results_pruned/checkpoint-70/tokenizer_config.json\n",
            "Special tokens file saved in ./results_pruned/checkpoint-70/special_tokens_map.json\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 512\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results_pruned/checkpoint-75\n",
            "Configuration saved in ./results_pruned/checkpoint-75/config.json\n",
            "Model weights saved in ./results_pruned/checkpoint-75/model.safetensors\n",
            "tokenizer config file saved in ./results_pruned/checkpoint-75/tokenizer_config.json\n",
            "Special tokens file saved in ./results_pruned/checkpoint-75/special_tokens_map.json\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 512\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results_pruned/checkpoint-80\n",
            "Configuration saved in ./results_pruned/checkpoint-80/config.json\n",
            "Model weights saved in ./results_pruned/checkpoint-80/model.safetensors\n",
            "tokenizer config file saved in ./results_pruned/checkpoint-80/tokenizer_config.json\n",
            "Special tokens file saved in ./results_pruned/checkpoint-80/special_tokens_map.json\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 512\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results_pruned/checkpoint-85\n",
            "Configuration saved in ./results_pruned/checkpoint-85/config.json\n",
            "Model weights saved in ./results_pruned/checkpoint-85/model.safetensors\n",
            "tokenizer config file saved in ./results_pruned/checkpoint-85/tokenizer_config.json\n",
            "Special tokens file saved in ./results_pruned/checkpoint-85/special_tokens_map.json\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 512\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results_pruned/checkpoint-90\n",
            "Configuration saved in ./results_pruned/checkpoint-90/config.json\n",
            "Model weights saved in ./results_pruned/checkpoint-90/model.safetensors\n",
            "tokenizer config file saved in ./results_pruned/checkpoint-90/tokenizer_config.json\n",
            "Special tokens file saved in ./results_pruned/checkpoint-90/special_tokens_map.json\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 512\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results_pruned/checkpoint-95\n",
            "Configuration saved in ./results_pruned/checkpoint-95/config.json\n",
            "Model weights saved in ./results_pruned/checkpoint-95/model.safetensors\n",
            "tokenizer config file saved in ./results_pruned/checkpoint-95/tokenizer_config.json\n",
            "Special tokens file saved in ./results_pruned/checkpoint-95/special_tokens_map.json\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 512\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results_pruned/checkpoint-100\n",
            "Configuration saved in ./results_pruned/checkpoint-100/config.json\n",
            "Model weights saved in ./results_pruned/checkpoint-100/model.safetensors\n",
            "tokenizer config file saved in ./results_pruned/checkpoint-100/tokenizer_config.json\n",
            "Special tokens file saved in ./results_pruned/checkpoint-100/special_tokens_map.json\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 512\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results_pruned/checkpoint-105\n",
            "Configuration saved in ./results_pruned/checkpoint-105/config.json\n",
            "Model weights saved in ./results_pruned/checkpoint-105/model.safetensors\n",
            "tokenizer config file saved in ./results_pruned/checkpoint-105/tokenizer_config.json\n",
            "Special tokens file saved in ./results_pruned/checkpoint-105/special_tokens_map.json\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 512\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results_pruned/checkpoint-110\n",
            "Configuration saved in ./results_pruned/checkpoint-110/config.json\n",
            "Model weights saved in ./results_pruned/checkpoint-110/model.safetensors\n",
            "tokenizer config file saved in ./results_pruned/checkpoint-110/tokenizer_config.json\n",
            "Special tokens file saved in ./results_pruned/checkpoint-110/special_tokens_map.json\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 512\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results_pruned/checkpoint-115\n",
            "Configuration saved in ./results_pruned/checkpoint-115/config.json\n",
            "Model weights saved in ./results_pruned/checkpoint-115/model.safetensors\n",
            "tokenizer config file saved in ./results_pruned/checkpoint-115/tokenizer_config.json\n",
            "Special tokens file saved in ./results_pruned/checkpoint-115/special_tokens_map.json\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 512\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results_pruned/checkpoint-120\n",
            "Configuration saved in ./results_pruned/checkpoint-120/config.json\n",
            "Model weights saved in ./results_pruned/checkpoint-120/model.safetensors\n",
            "tokenizer config file saved in ./results_pruned/checkpoint-120/tokenizer_config.json\n",
            "Special tokens file saved in ./results_pruned/checkpoint-120/special_tokens_map.json\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 512\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results_pruned/checkpoint-125\n",
            "Configuration saved in ./results_pruned/checkpoint-125/config.json\n",
            "Model weights saved in ./results_pruned/checkpoint-125/model.safetensors\n",
            "tokenizer config file saved in ./results_pruned/checkpoint-125/tokenizer_config.json\n",
            "Special tokens file saved in ./results_pruned/checkpoint-125/special_tokens_map.json\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 512\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results_pruned/checkpoint-130\n",
            "Configuration saved in ./results_pruned/checkpoint-130/config.json\n",
            "Model weights saved in ./results_pruned/checkpoint-130/model.safetensors\n",
            "tokenizer config file saved in ./results_pruned/checkpoint-130/tokenizer_config.json\n",
            "Special tokens file saved in ./results_pruned/checkpoint-130/special_tokens_map.json\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 512\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results_pruned/checkpoint-135\n",
            "Configuration saved in ./results_pruned/checkpoint-135/config.json\n",
            "Model weights saved in ./results_pruned/checkpoint-135/model.safetensors\n",
            "tokenizer config file saved in ./results_pruned/checkpoint-135/tokenizer_config.json\n",
            "Special tokens file saved in ./results_pruned/checkpoint-135/special_tokens_map.json\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 512\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results_pruned/checkpoint-140\n",
            "Configuration saved in ./results_pruned/checkpoint-140/config.json\n",
            "Model weights saved in ./results_pruned/checkpoint-140/model.safetensors\n",
            "tokenizer config file saved in ./results_pruned/checkpoint-140/tokenizer_config.json\n",
            "Special tokens file saved in ./results_pruned/checkpoint-140/special_tokens_map.json\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 512\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results_pruned/checkpoint-145\n",
            "Configuration saved in ./results_pruned/checkpoint-145/config.json\n",
            "Model weights saved in ./results_pruned/checkpoint-145/model.safetensors\n",
            "tokenizer config file saved in ./results_pruned/checkpoint-145/tokenizer_config.json\n",
            "Special tokens file saved in ./results_pruned/checkpoint-145/special_tokens_map.json\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 512\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results_pruned/checkpoint-150\n",
            "Configuration saved in ./results_pruned/checkpoint-150/config.json\n",
            "Model weights saved in ./results_pruned/checkpoint-150/model.safetensors\n",
            "tokenizer config file saved in ./results_pruned/checkpoint-150/tokenizer_config.json\n",
            "Special tokens file saved in ./results_pruned/checkpoint-150/special_tokens_map.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from ./results_pruned/checkpoint-65 (score: 0.659418523311615).\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=150, training_loss=0.5375990931193034, metrics={'train_runtime': 382.3186, 'train_samples_per_second': 195.387, 'train_steps_per_second': 0.392, 'total_flos': 1.9654395835392e+16, 'train_loss': 0.5375990931193034, 'epoch': 30.0})"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['sentence1'], examples['sentence2'], padding=\"max_length\", truncation=True, max_length=512)\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results_pruned',\n",
        "    num_train_epochs=30,\n",
        "    per_device_train_batch_size=64,\n",
        "    per_device_eval_batch_size=64,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs_pruned',\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True\n",
        ")\n",
        "\n",
        "# Initialize the Trainer with the pruned model\n",
        "trainer = Trainer(\n",
        "    model=pruned_model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets['train'],\n",
        "    eval_dataset=tokenized_datasets['validation'],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# Train the pruned model\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8ec6469-3cd9-42b6-95fe-cd9026421329",
      "metadata": {
        "id": "d8ec6469-3cd9-42b6-95fe-cd9026421329",
        "outputId": "47d4dbc7-ebc4-4318-d269-d3e1d6513762",
        "colab": {
          "referenced_widgets": [
            "db5ce75c52024055aeb7b2dbb911482c",
            "4b18e9f459074fb296e18bcbae56d71b",
            "6f8e49afb7f84aa1ab02cfad833ac1b2",
            "b313985198fe4ee1a1a1212eb7b5f672",
            "5071f5f08fc24c3d9ea1b03f0064e322",
            "86d5bbe96fe14242b30f32bc215b1bcc",
            "a68ec724f88041c2a8dc294a8eac8a0b"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "db5ce75c52024055aeb7b2dbb911482c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4b18e9f459074fb296e18bcbae56d71b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6f8e49afb7f84aa1ab02cfad833ac1b2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading file vocab.json from cache at /nethome/dsanyal7/.cache/huggingface/hub/models--microsoft--deberta-large/snapshots/a97e054da5f34feed3d26951db4a25831dfcb486/vocab.json\n",
            "loading file merges.txt from cache at /nethome/dsanyal7/.cache/huggingface/hub/models--microsoft--deberta-large/snapshots/a97e054da5f34feed3d26951db4a25831dfcb486/merges.txt\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at /nethome/dsanyal7/.cache/huggingface/hub/models--microsoft--deberta-large/snapshots/a97e054da5f34feed3d26951db4a25831dfcb486/tokenizer_config.json\n",
            "loading file tokenizer.json from cache at None\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b313985198fe4ee1a1a1212eb7b5f672",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/475 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file config.json from cache at /nethome/dsanyal7/.cache/huggingface/hub/models--microsoft--deberta-large/snapshots/a97e054da5f34feed3d26951db4a25831dfcb486/config.json\n",
            "Model config DebertaConfig {\n",
            "  \"_name_or_path\": \"microsoft/deberta-large\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-07,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"max_relative_positions\": -1,\n",
            "  \"model_type\": \"deberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_dropout\": 0,\n",
            "  \"pooler_hidden_act\": \"gelu\",\n",
            "  \"pooler_hidden_size\": 1024,\n",
            "  \"pos_att_type\": [\n",
            "    \"c2p\",\n",
            "    \"p2c\"\n",
            "  ],\n",
            "  \"position_biased_input\": false,\n",
            "  \"relative_attention\": true,\n",
            "  \"transformers_version\": \"4.40.1\",\n",
            "  \"type_vocab_size\": 0,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5071f5f08fc24c3d9ea1b03f0064e322",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/2490 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "86d5bbe96fe14242b30f32bc215b1bcc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/277 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a68ec724f88041c2a8dc294a8eac8a0b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import DebertaTokenizer\n",
        "\n",
        "roberta_tokenizer = DebertaTokenizer.from_pretrained('microsoft/deberta-large')\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return roberta_tokenizer(examples['sentence1'], examples['sentence2'], padding=\"max_length\", truncation=True, max_length=256)\n",
        "\n",
        "roberta_encoded_dataset = dataset.map(tokenize_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79ac3c15-0257-48bc-8c19-88942d9927dc",
      "metadata": {
        "id": "79ac3c15-0257-48bc-8c19-88942d9927dc",
        "outputId": "1be6f165-1905-4d2a-fa05-2c9b85d866fe",
        "colab": {
          "referenced_widgets": [
            "390a2eb784824dc7a4689a240367fc17"
          ]
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file config.json from cache at /nethome/dsanyal7/.cache/huggingface/hub/models--microsoft--deberta-large/snapshots/a97e054da5f34feed3d26951db4a25831dfcb486/config.json\n",
            "Model config DebertaConfig {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-07,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"max_relative_positions\": -1,\n",
            "  \"model_type\": \"deberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_dropout\": 0,\n",
            "  \"pooler_hidden_act\": \"gelu\",\n",
            "  \"pooler_hidden_size\": 1024,\n",
            "  \"pos_att_type\": [\n",
            "    \"c2p\",\n",
            "    \"p2c\"\n",
            "  ],\n",
            "  \"position_biased_input\": false,\n",
            "  \"relative_attention\": true,\n",
            "  \"transformers_version\": \"4.40.1\",\n",
            "  \"type_vocab_size\": 0,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "390a2eb784824dc7a4689a240367fc17",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading weights file pytorch_model.bin from cache at /nethome/dsanyal7/.cache/huggingface/hub/models--microsoft--deberta-large/snapshots/a97e054da5f34feed3d26951db4a25831dfcb486/pytorch_model.bin\n",
            "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']\n",
            "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import DebertaForSequenceClassification\n",
        "\n",
        "teacher_model = DebertaForSequenceClassification.from_pretrained('microsoft/deberta-large', num_labels=2).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd6a39ce-8793-43b7-9c54-360cfdb7aeca",
      "metadata": {
        "id": "dd6a39ce-8793-43b7-9c54-360cfdb7aeca",
        "outputId": "c49b20b1-05de-4b7f-b651-00f5c89f57ae"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
            "The following columns in the training set don't have a corresponding argument in `DebertaForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `DebertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running training *****\n",
            "  Num examples = 2,490\n",
            "  Num Epochs = 30\n",
            "  Instantaneous batch size per device = 16\n",
            "  Training with DataParallel so batch size has been adjusted to: 128\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 600\n",
            "  Number of trainable parameters = 406,214,658\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='288' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [288/600 07:44 < 08:27, 0.62 it/s, Epoch 14.35/30]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.691600</td>\n",
              "      <td>0.689720</td>\n",
              "      <td>0.545126</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.688000</td>\n",
              "      <td>0.675311</td>\n",
              "      <td>0.617329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.667700</td>\n",
              "      <td>0.609893</td>\n",
              "      <td>0.689531</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.514500</td>\n",
              "      <td>0.556846</td>\n",
              "      <td>0.754513</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.349700</td>\n",
              "      <td>0.668946</td>\n",
              "      <td>0.768953</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.208600</td>\n",
              "      <td>0.574032</td>\n",
              "      <td>0.830325</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.154600</td>\n",
              "      <td>0.520581</td>\n",
              "      <td>0.855596</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.074300</td>\n",
              "      <td>0.675556</td>\n",
              "      <td>0.848375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.079200</td>\n",
              "      <td>0.705839</td>\n",
              "      <td>0.826715</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.053700</td>\n",
              "      <td>0.641292</td>\n",
              "      <td>0.859206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.072200</td>\n",
              "      <td>0.839933</td>\n",
              "      <td>0.859206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.052000</td>\n",
              "      <td>1.012122</td>\n",
              "      <td>0.837545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.061600</td>\n",
              "      <td>0.849836</td>\n",
              "      <td>0.848375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.090000</td>\n",
              "      <td>0.858780</td>\n",
              "      <td>0.830325</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `DebertaForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `DebertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 128\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results/checkpoint-20\n",
            "Configuration saved in ./results/checkpoint-20/config.json\n",
            "Model weights saved in ./results/checkpoint-20/model.safetensors\n",
            "tokenizer config file saved in ./results/checkpoint-20/tokenizer_config.json\n",
            "Special tokens file saved in ./results/checkpoint-20/special_tokens_map.json\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "The following columns in the evaluation set don't have a corresponding argument in `DebertaForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `DebertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 128\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results/checkpoint-40\n",
            "Configuration saved in ./results/checkpoint-40/config.json\n",
            "Model weights saved in ./results/checkpoint-40/model.safetensors\n",
            "tokenizer config file saved in ./results/checkpoint-40/tokenizer_config.json\n",
            "Special tokens file saved in ./results/checkpoint-40/special_tokens_map.json\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "The following columns in the evaluation set don't have a corresponding argument in `DebertaForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `DebertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 128\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results/checkpoint-60\n",
            "Configuration saved in ./results/checkpoint-60/config.json\n",
            "Model weights saved in ./results/checkpoint-60/model.safetensors\n",
            "tokenizer config file saved in ./results/checkpoint-60/tokenizer_config.json\n",
            "Special tokens file saved in ./results/checkpoint-60/special_tokens_map.json\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "The following columns in the evaluation set don't have a corresponding argument in `DebertaForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `DebertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 128\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results/checkpoint-80\n",
            "Configuration saved in ./results/checkpoint-80/config.json\n",
            "Model weights saved in ./results/checkpoint-80/model.safetensors\n",
            "tokenizer config file saved in ./results/checkpoint-80/tokenizer_config.json\n",
            "Special tokens file saved in ./results/checkpoint-80/special_tokens_map.json\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "The following columns in the evaluation set don't have a corresponding argument in `DebertaForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `DebertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 128\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results/checkpoint-100\n",
            "Configuration saved in ./results/checkpoint-100/config.json\n",
            "Model weights saved in ./results/checkpoint-100/model.safetensors\n",
            "tokenizer config file saved in ./results/checkpoint-100/tokenizer_config.json\n",
            "Special tokens file saved in ./results/checkpoint-100/special_tokens_map.json\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "The following columns in the evaluation set don't have a corresponding argument in `DebertaForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `DebertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 128\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results/checkpoint-120\n",
            "Configuration saved in ./results/checkpoint-120/config.json\n",
            "Model weights saved in ./results/checkpoint-120/model.safetensors\n",
            "tokenizer config file saved in ./results/checkpoint-120/tokenizer_config.json\n",
            "Special tokens file saved in ./results/checkpoint-120/special_tokens_map.json\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "The following columns in the evaluation set don't have a corresponding argument in `DebertaForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `DebertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 128\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results/checkpoint-140\n",
            "Configuration saved in ./results/checkpoint-140/config.json\n",
            "Model weights saved in ./results/checkpoint-140/model.safetensors\n",
            "tokenizer config file saved in ./results/checkpoint-140/tokenizer_config.json\n",
            "Special tokens file saved in ./results/checkpoint-140/special_tokens_map.json\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "The following columns in the evaluation set don't have a corresponding argument in `DebertaForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `DebertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 128\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results/checkpoint-160\n",
            "Configuration saved in ./results/checkpoint-160/config.json\n",
            "Model weights saved in ./results/checkpoint-160/model.safetensors\n",
            "tokenizer config file saved in ./results/checkpoint-160/tokenizer_config.json\n",
            "Special tokens file saved in ./results/checkpoint-160/special_tokens_map.json\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "The following columns in the evaluation set don't have a corresponding argument in `DebertaForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `DebertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 128\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results/checkpoint-180\n",
            "Configuration saved in ./results/checkpoint-180/config.json\n",
            "Model weights saved in ./results/checkpoint-180/model.safetensors\n",
            "tokenizer config file saved in ./results/checkpoint-180/tokenizer_config.json\n",
            "Special tokens file saved in ./results/checkpoint-180/special_tokens_map.json\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "The following columns in the evaluation set don't have a corresponding argument in `DebertaForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `DebertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 128\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results/checkpoint-200\n",
            "Configuration saved in ./results/checkpoint-200/config.json\n",
            "Model weights saved in ./results/checkpoint-200/model.safetensors\n",
            "tokenizer config file saved in ./results/checkpoint-200/tokenizer_config.json\n",
            "Special tokens file saved in ./results/checkpoint-200/special_tokens_map.json\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "The following columns in the evaluation set don't have a corresponding argument in `DebertaForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `DebertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 128\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results/checkpoint-220\n",
            "Configuration saved in ./results/checkpoint-220/config.json\n",
            "Model weights saved in ./results/checkpoint-220/model.safetensors\n",
            "tokenizer config file saved in ./results/checkpoint-220/tokenizer_config.json\n",
            "Special tokens file saved in ./results/checkpoint-220/special_tokens_map.json\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "The following columns in the evaluation set don't have a corresponding argument in `DebertaForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `DebertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 128\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results/checkpoint-240\n",
            "Configuration saved in ./results/checkpoint-240/config.json\n",
            "Model weights saved in ./results/checkpoint-240/model.safetensors\n",
            "tokenizer config file saved in ./results/checkpoint-240/tokenizer_config.json\n",
            "Special tokens file saved in ./results/checkpoint-240/special_tokens_map.json\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "The following columns in the evaluation set don't have a corresponding argument in `DebertaForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `DebertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 128\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results/checkpoint-260\n",
            "Configuration saved in ./results/checkpoint-260/config.json\n",
            "Model weights saved in ./results/checkpoint-260/model.safetensors\n",
            "tokenizer config file saved in ./results/checkpoint-260/tokenizer_config.json\n",
            "Special tokens file saved in ./results/checkpoint-260/special_tokens_map.json\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "The following columns in the evaluation set don't have a corresponding argument in `DebertaForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `DebertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 128\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results/checkpoint-280\n",
            "Configuration saved in ./results/checkpoint-280/config.json\n",
            "Model weights saved in ./results/checkpoint-280/model.safetensors\n",
            "tokenizer config file saved in ./results/checkpoint-280/tokenizer_config.json\n",
            "Special tokens file saved in ./results/checkpoint-280/special_tokens_map.json\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "File \u001b[0;32m/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/accelerate/utils/operations.py:158\u001b[0m, in \u001b[0;36msend_to_device\u001b[0;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:  \u001b[38;5;66;03m# .to() doesn't accept non_blocking as kwarg\u001b[39;00m\n",
            "\u001b[0;31mTypeError\u001b[0m: BatchEncoding.to() got an unexpected keyword argument 'non_blocking'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[27], line 27\u001b[0m\n\u001b[1;32m     17\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     18\u001b[0m     model\u001b[38;5;241m=\u001b[39mteacher_model,\n\u001b[1;32m     19\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics  \n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/transformers/trainer.py:1859\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1857\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1858\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1860\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1861\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1862\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1863\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1864\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/transformers/trainer.py:2165\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2162\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2164\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 2165\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   2166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_batched_samples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m   2168\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minclude_num_input_tokens_seen\u001b[49m\u001b[43m:\u001b[49m\n",
            "File \u001b[0;32m/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/accelerate/data_loader.py:461\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;66;03m# But we still move it to the device so it is done before `StopIteration` is reached\u001b[39;00m\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 461\u001b[0m         current_batch \u001b[38;5;241m=\u001b[39m \u001b[43msend_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    462\u001b[0m     next_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(dataloader_iter)\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m batch_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_batches:\n",
            "File \u001b[0;32m/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/accelerate/utils/operations.py:160\u001b[0m, in \u001b[0;36msend_to_device\u001b[0;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mto(device, non_blocking\u001b[38;5;241m=\u001b[39mnon_blocking)\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:  \u001b[38;5;66;03m# .to() doesn't accept non_blocking as kwarg\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# `torch.Tensor.to(<int num>)` is not supported by `torch_npu` (see this [issue](https://github.com/Ascend/pytorch/issues/16)).\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# This call is inside the try-block since is_npu_available is not supported by torch.compile.\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_npu_available():\n",
            "File \u001b[0;32m/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:800\u001b[0m, in \u001b[0;36mBatchEncoding.to\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    796\u001b[0m \u001b[38;5;66;03m# This check catches things like APEX blindly calling \"to\" on all inputs to a module\u001b[39;00m\n\u001b[1;32m    797\u001b[0m \u001b[38;5;66;03m# Otherwise it passes the casts down and casts the LongTensor containing the token idxs\u001b[39;00m\n\u001b[1;32m    798\u001b[0m \u001b[38;5;66;03m# into a HalfTensor\u001b[39;00m\n\u001b[1;32m    799\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m is_torch_device(device) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m--> 800\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m {k: \u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    801\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    802\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to cast a BatchEncoding to type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(device)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=30,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    log_level='info',\n",
        "    load_best_model_at_end=True\n",
        ")\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=teacher_model,\n",
        "    args=training_args,\n",
        "    train_dataset=roberta_encoded_dataset['train'],\n",
        "    eval_dataset=roberta_encoded_dataset['validation'],\n",
        "    tokenizer=roberta_tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "821e0441-d899-4cdf-b79f-c21d298fd404",
      "metadata": {
        "id": "821e0441-d899-4cdf-b79f-c21d298fd404",
        "outputId": "d9922105-bf85-4339-bb7b-4ae54c26003f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.nn import KLDivLoss, CrossEntropyLoss, Softmax, LogSoftmax\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "teacher_tokenizer = roberta_tokenizer\n",
        "student_tokenizer = tokenizer\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    # print(examples)\n",
        "    teacher_encodings = teacher_tokenizer(examples['sentence1'], examples['sentence2'], truncation=True, padding='max_length', max_length=256)\n",
        "    student_encodings = student_tokenizer(examples['sentence1'], examples['sentence2'], truncation=True, padding='max_length', max_length=256)\n",
        "    answer = {\n",
        "        \"input_ids_teacher\": teacher_encodings['input_ids'],\n",
        "        \"attention_mask_teacher\": teacher_encodings['attention_mask'],\n",
        "        \"input_ids_student\": student_encodings['input_ids'],\n",
        "        \"attention_mask_student\": student_encodings['attention_mask'],\n",
        "        \"label\": examples[\"label\"]\n",
        "    }\n",
        "    # print(answer)\n",
        "    return answer\n",
        "\n",
        "# Apply tokenization to dataset\n",
        "# tokenized_datasets = tokenize_function(next(dataset))\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "# for a in tokenized_datasets[\"train\"]:\n",
        "#     print(\"-----\")\n",
        "#     print(a)\n",
        "#     print(\"-----\")\n",
        "#     break\n",
        "print()\n",
        "\n",
        "teacher_model.eval()\n",
        "\n",
        "def distillation_loss(teacher_logits, student_logits, labels, T=2.0, alpha=0.5):\n",
        "    \"\"\" Compute the distillation loss. \"\"\"\n",
        "    ce_loss = CrossEntropyLoss()(student_logits, labels)\n",
        "    kl_loss = KLDivLoss(reduction=\"batchmean\")(LogSoftmax(dim=-1)(student_logits/T), Softmax(dim=-1)(teacher_logits/T))\n",
        "    return alpha * kl_loss * (T * T) + (1.0 - alpha) * ce_loss\n",
        "\n",
        "# def compute_loss(model, inputs, return_outputs=False):\n",
        "#     \"\"\" Custom loss computation for distillation. \"\"\"\n",
        "#     outputs_student = model(**inputs)\n",
        "#     with torch.no_grad():\n",
        "#         outputs_teacher = teacher_model(**inputs)\n",
        "#     loss = distillation_loss(outputs_teacher.logits, outputs_student.logits, inputs[\"labels\"])\n",
        "#     return (loss, outputs_student) if return_outputs else loss\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results_distil',\n",
        "    num_train_epochs=30,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    logging_dir='./logs_distil',\n",
        "    evaluation_strategy=\"epoch\",\n",
        "     save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    log_level='info',\n",
        "    logging_steps=10,\n",
        "    warmup_steps=50,\n",
        "    weight_decay=0.01,\n",
        "    remove_unused_columns = False\n",
        ")\n",
        "\n",
        "from transformers import Trainer, TrainingArguments\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "teacher_model.to(device)\n",
        "\n",
        "class DistilTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        # Prepare inputs for both student and teacher models\n",
        "        # print(inputs)\n",
        "        student_inputs = {\n",
        "            'input_ids': inputs['input_ids_student'],\n",
        "            'attention_mask': inputs['attention_mask_student']\n",
        "        }\n",
        "        teacher_inputs = {\n",
        "            'input_ids': inputs['input_ids_teacher'],\n",
        "            'attention_mask': inputs['attention_mask_teacher']\n",
        "        }\n",
        "\n",
        "        # Pass the relevant keys to each model\n",
        "        outputs_student = model(**student_inputs)\n",
        "        with torch.no_grad():\n",
        "            outputs_teacher = teacher_model(**teacher_inputs)\n",
        "\n",
        "        # Compute the distillation loss\n",
        "        # Assuming alpha and temperature are properly defined\n",
        "        alpha = 0.5\n",
        "        temperature = 5\n",
        "        loss_logits = nn.KLDivLoss(reduction=\"batchmean\")(\n",
        "            F.log_softmax(outputs_student.logits / temperature, dim=-1),\n",
        "            F.softmax(outputs_teacher.logits / temperature, dim=-1)) * (temperature ** 2)\n",
        "\n",
        "        student_loss = CrossEntropyLoss()(outputs_student.logits, inputs[\"labels\"])\n",
        "        loss = alpha * student_loss + (1. - alpha) * loss_logits\n",
        "        return (loss, outputs_student) if return_outputs else loss\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3f8d308-8058-426e-957a-5485cd51ec98",
      "metadata": {
        "id": "a3f8d308-8058-426e-957a-5485cd51ec98",
        "outputId": "41cc6d1f-1817-4465-db50-99814fe19ca1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
            "***** Running training *****\n",
            "  Num examples = 2,490\n",
            "  Num Epochs = 30\n",
            "  Instantaneous batch size per device = 32\n",
            "  Training with DataParallel so batch size has been adjusted to: 256\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 300\n",
            "  Number of trainable parameters = 109,483,778\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='154' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [154/300 13:18 < 12:47, 0.19 it/s, Epoch 15.30/30]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.186800</td>\n",
              "      <td>2.453014</td>\n",
              "      <td>0.570397</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.150000</td>\n",
              "      <td>2.576062</td>\n",
              "      <td>0.566787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.116800</td>\n",
              "      <td>2.789143</td>\n",
              "      <td>0.545126</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.089800</td>\n",
              "      <td>3.106702</td>\n",
              "      <td>0.548736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.101300</td>\n",
              "      <td>3.266970</td>\n",
              "      <td>0.548736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.094100</td>\n",
              "      <td>3.249267</td>\n",
              "      <td>0.541516</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.085800</td>\n",
              "      <td>3.224790</td>\n",
              "      <td>0.555957</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.119800</td>\n",
              "      <td>3.221021</td>\n",
              "      <td>0.577617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.095900</td>\n",
              "      <td>3.174405</td>\n",
              "      <td>0.548736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.076100</td>\n",
              "      <td>3.363716</td>\n",
              "      <td>0.516245</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.113000</td>\n",
              "      <td>3.275368</td>\n",
              "      <td>0.541516</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.165500</td>\n",
              "      <td>3.239366</td>\n",
              "      <td>0.555957</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.084900</td>\n",
              "      <td>3.283078</td>\n",
              "      <td>0.555957</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.082400</td>\n",
              "      <td>3.370773</td>\n",
              "      <td>0.537906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.080000</td>\n",
              "      <td>3.181747</td>\n",
              "      <td>0.534296</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 256\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results_distil/checkpoint-10\n",
            "Configuration saved in ./results_distil/checkpoint-10/config.json\n",
            "Model weights saved in ./results_distil/checkpoint-10/model.safetensors\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 256\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results_distil/checkpoint-20\n",
            "Configuration saved in ./results_distil/checkpoint-20/config.json\n",
            "Model weights saved in ./results_distil/checkpoint-20/model.safetensors\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 256\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results_distil/checkpoint-30\n",
            "Configuration saved in ./results_distil/checkpoint-30/config.json\n",
            "Model weights saved in ./results_distil/checkpoint-30/model.safetensors\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 256\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results_distil/checkpoint-40\n",
            "Configuration saved in ./results_distil/checkpoint-40/config.json\n",
            "Model weights saved in ./results_distil/checkpoint-40/model.safetensors\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 256\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results_distil/checkpoint-50\n",
            "Configuration saved in ./results_distil/checkpoint-50/config.json\n",
            "Model weights saved in ./results_distil/checkpoint-50/model.safetensors\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 256\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results_distil/checkpoint-60\n",
            "Configuration saved in ./results_distil/checkpoint-60/config.json\n",
            "Model weights saved in ./results_distil/checkpoint-60/model.safetensors\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 256\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results_distil/checkpoint-70\n",
            "Configuration saved in ./results_distil/checkpoint-70/config.json\n",
            "Model weights saved in ./results_distil/checkpoint-70/model.safetensors\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 256\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results_distil/checkpoint-80\n",
            "Configuration saved in ./results_distil/checkpoint-80/config.json\n",
            "Model weights saved in ./results_distil/checkpoint-80/model.safetensors\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 256\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results_distil/checkpoint-90\n",
            "Configuration saved in ./results_distil/checkpoint-90/config.json\n",
            "Model weights saved in ./results_distil/checkpoint-90/model.safetensors\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 256\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results_distil/checkpoint-100\n",
            "Configuration saved in ./results_distil/checkpoint-100/config.json\n",
            "Model weights saved in ./results_distil/checkpoint-100/model.safetensors\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 256\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results_distil/checkpoint-110\n",
            "Configuration saved in ./results_distil/checkpoint-110/config.json\n",
            "Model weights saved in ./results_distil/checkpoint-110/model.safetensors\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 256\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results_distil/checkpoint-120\n",
            "Configuration saved in ./results_distil/checkpoint-120/config.json\n",
            "Model weights saved in ./results_distil/checkpoint-120/model.safetensors\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 256\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results_distil/checkpoint-130\n",
            "Configuration saved in ./results_distil/checkpoint-130/config.json\n",
            "Model weights saved in ./results_distil/checkpoint-130/model.safetensors\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 256\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results_distil/checkpoint-140\n",
            "Configuration saved in ./results_distil/checkpoint-140/config.json\n",
            "Model weights saved in ./results_distil/checkpoint-140/model.safetensors\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 277\n",
            "  Batch size = 256\n",
            "/coc/scratch/debopam/env/envs/graphing/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to ./results_distil/checkpoint-150\n",
            "Configuration saved in ./results_distil/checkpoint-150/config.json\n",
            "Model weights saved in ./results_distil/checkpoint-150/model.safetensors\n"
          ]
        }
      ],
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "import torch\n",
        "\n",
        "class CustomDataCollator(DataCollatorWithPadding):\n",
        "    def __init__(self, tokenizer, return_tensors=\"pt\"):\n",
        "        super().__init__(tokenizer=tokenizer, return_tensors=return_tensors)\n",
        "\n",
        "    def __call__(self, features):\n",
        "        # Use superclass to handle input_ids, attention_mask, etc.\n",
        "        # print(features)\n",
        "        batch = super().__call__(features)\n",
        "\n",
        "        # Check and print to debug\n",
        "        print(\"Batch keys after super call:\", batch.keys())\n",
        "\n",
        "        # Ensure custom handling for your specific fields\n",
        "        if 'input_ids_student' in features[0]:  # Check if your key exists in the feature set\n",
        "            batch['input_ids_student'] = torch.stack([f['input_ids_student'] for f in features])\n",
        "            batch['attention_mask_student'] = torch.stack([f['attention_mask_student'] for f in features])\n",
        "        if 'input_ids_teacher' in features[0]:\n",
        "            batch['input_ids_teacher'] = torch.stack([f['input_ids_teacher'] for f in features])\n",
        "            batch['attention_mask_teacher'] = torch.stack([f['attention_mask_teacher'] for f in features])\n",
        "\n",
        "        return batch\n",
        "\n",
        "data_collator = CustomDataCollator(tokenizer=student_tokenizer)\n",
        "\n",
        "# for a in tokenized_datasets[\"train\"]:\n",
        "#     print(\"-----\")\n",
        "#     print(a)\n",
        "#     print(\"-----\")\n",
        "#     break\n",
        "# print()\n",
        "\n",
        "trainer = DistilTrainer(\n",
        "    model=pruned_model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets['train'],\n",
        "    eval_dataset=tokenized_datasets['validation'],\n",
        "    compute_metrics=compute_metrics\n",
        "    # data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# Train the student model\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d05a25d0-6655-476d-8bac-7e5df621ddc0",
      "metadata": {
        "id": "d05a25d0-6655-476d-8bac-7e5df621ddc0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "486798cc-611a-46c8-8836-49a7f4dc0549",
      "metadata": {
        "id": "486798cc-611a-46c8-8836-49a7f4dc0549"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}